{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 熵\n",
    "\n",
    "Entropy:度量事物的不确定性，不确定性越高，熵就越大，反之越确定，熵越小；\n",
    "\n",
    "随机事件的熵可以表示为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "entropy = -p_1log_2{(p1)} - p_2log_2(p2) - ... - p_nlog_2(p_n) = -\\sum_{i=1}^n p_ilog_2(p_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息增益：\n",
    "\n",
    "父结点的熵与子结点上的平均值之间的差值；\n",
    "\n",
    "![](./dt001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 信息增益最大化\n",
    "\n",
    "**每次决策都选择信息增益最大化的特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11260735516748954\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def two_group_ent(first, tot):                        \n",
    "    return -(first/tot*np.log2(first/tot) + (tot-first)/tot*np.log2((tot-first)/tot))\n",
    "\n",
    "tot_ent = two_group_ent(10, 24)                       \n",
    "g17_ent = 15/24 * two_group_ent(11,15) + 9/24 * two_group_ent(6,9)                  \n",
    "\n",
    "answer = tot_ent - g17_ent  \n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树的超参数\n",
    "\n",
    "为了创建泛化能力好的决策树，我们可以调优决策树的多个方面。我们把决策树的这些可调优的多个方面称为“超参数”。以下是决策树中使用的一些最重要的超参数。\n",
    "\n",
    "## 最大深度\n",
    "决策树的最大深度就是从根到叶之间可能的最大长度。一个最大深度为 k 的决策树最多有$2^k$ 个叶子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最少样本分割数\n",
    "\n",
    "一个节点必须至少有min_samples_split个样本才能足够大以进行拆分。如果一个节点的样本数少于 min_samples_split 个， 则分割过程停止，该节点不会被分割。\n",
    "![](./dt002.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而 min_samples_split 不会控制叶的最小尺寸。正如你在上面右边的示例中看到的，父节点有20个样本，大于min_samples_split = 11，因此这个节点被拆分。但在此节点被拆分后，有一个子节点的样本数为5，小于min_samples_split = 11。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每片叶子的最小样本数\n",
    "\n",
    "当分割一个节点时，可能会遇到的一个问题是分割不均匀，例如某个子节点有99个样本，另一个子节点可能只有1个样本。这会影响决策树的生成，也浪费计算资源和时间。为避免这种情况，我们可以为每个叶子上允许的样本数设置一个最小值。\n",
    "\n",
    "## 每次分裂的最小样本数\n",
    "\n",
    "这个参数与每片叶子上的最小样本树相同，只不过是应用在节点的分裂当中。\n",
    "\n",
    "\n",
    "## 最大特征数\n",
    "有时，我们会遇到特征数量过于庞大，而无法建立决策树的情况。在这种状况下，对于每一个分裂，我们都需要检查整个数据集中的每一个特征。这种过程极为繁琐。而解决方案之一是限制每个分裂中查找的特征数。如果这个数字足够庞大，我们很有可能在查找的特征中找到良好特征（尽管也许并不是完美特征）。然而，如果这个数字小于特征数，这将极大加快我们的计算速度。\n",
    "\n",
    "![](./dt004.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn 中的决策树\n",
    "对于决策树模型，你将使用 scikit-learn 的 Decision Tree Classifier 类。该类提供了定义模型并将模型与数据进行拟合的函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-bff43a1554c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_values' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_values, y_values)\n",
    "print(model.predict([ [0.2, 0.8], [0.5, 0.4] ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 超参数\n",
    "当我们定义模型时，可以指定超参数。在实践中，最常见的超参数包括：\n",
    "\n",
    "max_depth：树中的最大层级数量。\n",
    "\n",
    "min_samples_leaf：叶子允许的最低样本数量。\n",
    "\n",
    "min_samples_split：拆分内部节点所需的最低样本数量。\n",
    "\n",
    "max_features：寻找最佳拆分方法时要考虑的特征数量。\n",
    "\n",
    "例如，在此例中，我们定义了一个模型：树的最大深度 max_depth 为7，每个叶子的最低元素数量 min_samples_leaf 是 10。\n",
    "\n",
    "model = DecisionTreeClassifier(max_depth = 7, min_samples_leaf = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建决策树步骤：\n",
    "\n",
    "## 1.构建决策树模型\n",
    "\n",
    "使用 scikit-learn 的 DecisionTree 类构建决策树分类模型，并将其赋值给变量 model。\n",
    "\n",
    "## 2.将模型与数据进行拟合\n",
    "\n",
    "你不需要指定任何超参数，因为默认的超参数将以 100% 的准确率拟合数据。\n",
    "\n",
    "但是，建议你实验这些超参数，例如 max_depth 和 min_samples_leaf，并尝试找到最简单的潜在模型，即最不太可能过拟合的模型！\n",
    "\n",
    "## 3.使用模型进行预测\n",
    "\n",
    "预测训练集的标签，并将此列表赋值给变量 y_pred。\n",
    "\n",
    "## 4.计算模型的准确率\n",
    "\n",
    "为此，使用 sklearn 函数 accuracy_score。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "代码实现:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the data.\n",
    "data = np.asarray(pd.read_csv('data.csv', header=None))\n",
    "# Assign the features to the variable X, and the labels to the variable y. \n",
    "X = data[:,0:2]\n",
    "y = data[:,2]\n",
    "\n",
    "# TODO: Create the decision tree model and assign it to the variable model.\n",
    "# You won't need to, but if you'd like, play with hyperparameters such\n",
    "# as max_depth and min_samples_leaf and see what they do to the decision\n",
    "# boundary.\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# TODO: Fit the model.\n",
    "model.fit(X,y)\n",
    "# TODO: Make predictions. Store them in the variable y_pred.\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# TODO: Calculate the accuracy and assign it to the variable acc.\n",
    "acc = accuracy_score(y_pred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
