{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多项式回归\n",
    "\n",
    "如果有一个预测器变量，线条的方程是\n",
    "\n",
    "y = m x + by=mx+b\n",
    "\n",
    "图形可能如下所示：\n",
    "\n",
    "![](./just-a-simple-lin-reg.png)\n",
    "\n",
    "添加一个预测器变量，变成两个预测器变量后，预测方程是\n",
    "\n",
    "$$\n",
    "y = m_1 x_1 + m_2 x_2 + by=m \n",
    "$$\n",
    "\n",
    "\n",
    "要用图形表示，我们需要三维图形，并将线性回归模型表示成一个平面：\n",
    "\n",
    "![](./just-a-2d-reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以使用两个以上的预测器变量，实际上可以使用任意多个，只要有用即可！如果你使用 nn 个预测器变量，那么模型可以用以下方程表示：\n",
    "\n",
    "$$\n",
    "y = m_{1} x_{1} + m_{2} x_{2} + m_{3} x_{3}+ ...+m_{n} x_{n} + by=m \n",
    "$$\n",
    "\n",
    "如果模型有多个预测器变量，则很难用图形呈现，但幸运的是，关于线性回归的所有其他方面都保持不变。我们依然可以通过相同的方式拟合模型并作出预测，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Load the data from the boston house-prices dataset \n",
    "boston_data = load_boston()\n",
    "x = boston_data['data']\n",
    "y = boston_data['target']\n",
    "\n",
    "# Make and fit the linear regression model\n",
    "# TODO: Fit the model and assign it to the model variable\n",
    "model = LinearRegression()\n",
    "model.fit(x,y)\n",
    "\n",
    "# Make a prediction using the model\n",
    "sample_house = [[2.29690000e-01, 0.00000000e+00, 1.05900000e+01, 0.00000000e+00, 4.89000000e-01,\n",
    "                6.32600000e+00, 5.25000000e+01, 4.35490000e+00, 4.00000000e+00, 2.77000000e+02,\n",
    "                1.86000000e+01, 3.94870000e+02, 1.09700000e+01]]\n",
    "# TODO: Predict housing price for the sample_house\n",
    "prediction = model.predict(sample_house)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.68284712]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 解数学方程组\n",
    "\n",
    "根据均方误差得到的数学式，要求误差函数最小值，只需要对每个未知参数求导，在误差最小处，所有偏导数为0,这样得到n个位置参数的n个方差组，解出方程组，即可得到拟合最佳参数。\n",
    "\n",
    "求n个方程组，需转置nxn矩阵，当n的个数非常大时，这个计算非常费时，这就是我们使用梯度下降法的原因，这种方法不能给出准确答案，但是可以无限接近最佳答案。\n",
    "\n",
    "![](./equations.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上诉过程，用矩阵来计算，可以得到以下式：\n",
    "$$\n",
    "W =(X^TX)^{-1}X^Ty\n",
    "$$\n",
    "\n",
    "正如上面说的，这种方法在现实生活中将会很昂贵而不实用，因为要找到矩阵的倒数 $X^TX$很难，如果是在 nxn 很大的情况下。 这就是我们需要经历多次梯度下降的痛苦的原因。\n",
    "\n",
    "但是，如果我们的**数据稀疏**，即如果矩阵的大部分条目 X 是 0，就会有一些非常有趣的算法，能够迅速找到这个倒数，将使这种方法在现实生活中非常有用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性回归注意事项\n",
    "\n",
    "线性回归会根据训练数据生成直线模型。如果训练数据包含非线性关系，你需要选择：调整数据（进行数据转换）、增加特征数量（参考下节内容）或改用其他模型。\n",
    "\n",
    "容易受到异常值影响\n",
    "线性回归的目标是求取对训练数据而言的 “最优拟合” 直线。如果数据集中存在不符合总体规律的异常值，最终结果将会存在不小偏差。\n",
    "\n",
    "在大多数情况下，模型需要基本上能与大部分数据拟合，所以要小心异常值！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多项式回归\n",
    "\n",
    "当出现以下数据，无法用直线拟合时，可以选择用多项式拟合，比如设置参数为$w_1,w_2,w_3,w_4$；\n",
    "\n",
    "我们只需要求出平均绝对误差或者均方误差，对4个变量求导，使用梯度下降法，修改4个权重，最小化误差。这种方法称为，多项式回归。\n",
    "\n",
    "![](./polynomial_regression.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Add import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Assign the data to predictor and outcome variables\n",
    "# TODO: Load the data\n",
    "train_data = pd.read_csv('data_polynomial.csv')\n",
    "X = train_data['Var_X'].values.reshape(-1, 1)\n",
    "y = train_data['Var_Y'].values\n",
    "\n",
    "# Create polynomial features\n",
    "# TODO: Create a PolynomialFeatures object, then fit and transform the\n",
    "# predictor feature\n",
    "#设定模型最高次数\n",
    "poly_feat = PolynomialFeatures(degree = 4)\n",
    "X_poly = poly_feat.fit_transform(X)\n",
    "\n",
    "# Make and fit the polynomial regression model\n",
    "# TODO: Create a LinearRegression object and fit it to the polynomial predictor\n",
    "# features\n",
    "poly_model = LinearRegression(fit_intercept = False).fit(X_poly, y)\n",
    "\n",
    "# Once you've completed all of the steps, select Test Run to see your model\n",
    "# predictions against the data, or select Submit Answer to check if the degree\n",
    "# of the polynomial features is the same as ours!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正则化\n",
    "\n",
    "左边模型简单，有小许错误，右边模型复杂，没有错误，而实际上左边简单模型泛化的更好，右边明显过拟合。\n",
    "\n",
    "问题是在训练时，右边复杂模型更容易出现，因为过拟合时，误差更小，那我们如何得到左侧的模型呢？\n",
    "\n",
    "![](./normal1.png)\n",
    "\n",
    "如图见，多元方程式更加复杂，包括高次项，而左边线性模型，更加简单，系数更少；\n",
    "\n",
    "如果我们能够通过这些参数的某个函数来表达这种误差，那将非常有用，因为在某种程度上，模型的复杂性将体现在这个函数中，一个复杂模型相较于简单模型将得到更大的误差。\n",
    "\n",
    "简单说，就是拿这些系数，加到误差里:\n",
    "\n",
    "## L1 Regularization\n",
    "\n",
    "L1正则化：把所有系数的绝对值加起来，加到误差里。\n",
    "\n",
    "reg01.png\n",
    "reg02.png\n",
    "\n",
    "如图见，这样复杂的模型，就得到较大的误差。这就是L1正则化，它与绝对值有关。\n",
    "\n",
    "\n",
    "## L2 Regularization\n",
    "\n",
    "L2正则化，是吧所有系数的平方加起来，加到误差里。\n",
    "\n",
    "lr01.png\n",
    "\n",
    "lr02.png\n",
    "\n",
    "我们再次看到复杂模型比简单模型受到更多“惩罚”。\n",
    "\n",
    "## $\\lambda$ 参数\n",
    "\n",
    "现有个新问题，如果我们的惩罚复杂模型太少或这太多怎么办？有些模型，比如火箭发射，医学模型，几乎没有容错的余地，那么我们接受一定的复杂性；\n",
    "\n",
    "而其他一些模型，比如视频推荐，社交网络潜在好友推荐等有更大的容错空间，需要模型简单从而更快地处理海量数据，因此可以接受一定误差。\n",
    "\n",
    "![](./lambda1.png)\n",
    "\n",
    "似乎对于每一个案例，我们必须调整要在多大程度上“惩罚”每个模型的复杂度，这个问题可以通过参数$\\lambda$来解决。\n",
    "\n",
    "用$\\lambda$来调整对复杂模型的“惩罚”力度：\n",
    "\n",
    "![](./lambda2.png)\n",
    "\n",
    "![](./lambda3.png)\n",
    "\n",
    "\n",
    "\n",
    "总而言之，如果我们的$\\lambda$值很大，那么将对模型复杂度有很大程度的惩罚，我们选择更简单的模型；\n",
    "\n",
    "反之，$\\lambda$越小，对模型惩罚小，选择更复杂的模型。\n",
    "\n",
    "![](./lambda4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 or L2的对比\n",
    "\n",
    "L1更简单，但是L2更容易求导，所以只有数据很稀疏时，L1才可能比L2快。\n",
    "\n",
    "L1有个好处，可以选择特征，将无用噪声列设置为零。\n",
    "\n",
    "![](./lambda5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 正则化练习\n",
    "\n",
    "sklearn中有一些类帮助将线性回归正则化。你将练习怎样实现将线性回归正则化。\n",
    "\n",
    "在附件的数据文件 (data.csv)中，你将看到一组数据点，包括6个预测器变量和1个结果变量。\n",
    "\n",
    "使用sklearn 的 Lasso 类，根据这些数据拟合一个线性回归模型，同时还使用L1正则化来控制模型的复杂性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          2.35793224  2.00441646 -0.05511954 -3.92808318  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Assign the data to predictor and outcome variables\n",
    "# TODO: Load the data\n",
    "train_data = pd.read_csv('lambda.csv', header = None)\n",
    "X = train_data.iloc[:,:-1]\n",
    "y = train_data.iloc[:,-1]\n",
    "\n",
    "# TODO: Create the linear regression model with lasso regularization.\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "\n",
    "# TODO: Fit the model.\n",
    "lasso_reg.fit(X, y)\n",
    "\n",
    "# TODO: Retrieve and print out the coefficients from the regression model.\n",
    "reg_coef = lasso_reg.coef_\n",
    "print(reg_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征缩放\n",
    "\n",
    "什么是特征缩放？特征缩放是把数据 (各个特征) 变换到同一个尺度。两种常见的缩放方法：\n",
    "\n",
    "1.标准化\n",
    "\n",
    "2.归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标准化\n",
    "标准化是对列中的每个值减去均值后再除以方差，即数据被转换为均值为0，标准差为1。\n",
    "\n",
    "在Python中，假设在 df 中有一列叫 height。可以用以下语句，创建一个标准化的高度：\n",
    "\n",
    "df[\"height_standard\"] = (df[\"height\"] - df[\"height\"].mean()) / df[\"height\"].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这将创建一个新的 \"标准化\" 列 。新列中的每一项都是原列值减去列均值后再除以列方差，新的标准化值可以解释为，原高度与平均高度之间相差多少个标准差。这是最常见的一种特征缩放技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 归一化\n",
    "\n",
    "第二种特征缩放方法是著名的归一化。归一化将数据压缩到0和1之间。仍使用上面标准化的例子，可以用下面的 Python 语句归一化数据：\n",
    "\n",
    "df[\"height_normal\"] = (df[\"height\"] - df[\"height\"].min()) /     \\\n",
    "                      (df[\"height\"].max() - df['height'].min())\n",
    "                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么时候做特征缩放?\n",
    "\n",
    "在许多机器学习算法中，数据缩放对预测结果的影响很大。尤其是在以下两个具体案例中：\n",
    "\n",
    "1.使用基于距离的特征做预测\n",
    "\n",
    "2.加入正则化\n",
    "\n",
    "#### 基于距离的特征\n",
    "\n",
    "在后面的课程中，你将看到一种基于距离点的常见监督学习技术支持向量机 (SVMs)。另一个用基于距离的方法是k近邻算法 (也称 k-nn)。当使用两种技术中的任何一种时，如果不对数据做特征缩放，可能会导致完全不同（也可能会误导）的预测结果。\n",
    "\n",
    "因此，用这些基于距离的技术做预测时，必须先进行特征缩放。\n",
    "\n",
    "#### 正则化\n",
    "当你开始在模型中使用正则化时，你将再次需要做特征缩放。特征的尺度对正则化线性回归技术中，正则化对特定系数的惩罚的影响很大。如果一个特征的取值区间是从0 到10，而另一个特征的取值区间是 0 到1000000， 不做特征缩放预处理，就应用正则化将不公平地惩罚小尺度的特征。相比大尺度特征，小尺度特征需要用更大的系数，才能对结果产生相同的影响（思考怎样让两个数 aa 和 bb 满足交换律，即 ab = baab=ba）。因此，在两个特征的净误差的增量相同情况下，正则化会删除那个系数大的小尺度特征，因为这将最大程度地减少正则化项。\n",
    "\n",
    "这再次说明，正则化前要先做特征缩放。\n",
    "\n",
    "关于使用正则化时特征缩放的重要性的一篇有用的文章。\n",
    "\n",
    "https://www.quora.com/Why-do-we-normalize-the-data\n",
    "\n",
    "这个文章中提到，特征缩放可以加快机器学习算法的收敛速度，这是扩展机器学习应用的一个重要的考虑因素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           3.90753617   9.02575748  -0.         -11.78303187\n",
      "   0.45340137]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add import statements\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assign the data to predictor and outcome variables\n",
    "# TODO: Load the data\n",
    "train_data = pd.read_csv('lambda.csv', header = None)\n",
    "X = train_data.iloc[:,:-1]\n",
    "y = train_data.iloc[:,-1]\n",
    "\n",
    "# TODO: Create the standardization scaling object.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# TODO: Fit the standardization parameters and scale the data.\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# TODO: Create the linear regression model with lasso regularization.\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "\n",
    "# TODO: Fit the model.\n",
    "lasso_reg.fit(X_scaled, y)\n",
    "\n",
    "# TODO: Retrieve and print out the coefficients from the regression model.\n",
    "reg_coef = lasso_reg.coef_\n",
    "print(reg_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小结\n",
    "\n",
    "本节课我们介绍了线性模型。具体来说，包括：\n",
    "\n",
    "用梯度下降 对线性模型做优化。\n",
    "\n",
    "如果有两个或两个以上的自变量，用多元线性回归 。\n",
    "\n",
    "用多项式回归 来描述非线性关系的变量.\n",
    "\n",
    "用正则化 来确保你的模型不仅训练误差小，而且测试误差也小 (泛化好)。\n",
    "\n",
    "结尾\n",
    "\n",
    "在本课中，我们学习了如何预测数值数据。预测数值型数据被认为是一个Regression问题。下节课，我们将介绍如何预测类别数据，这被称为是Classification 问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
