{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "误差函数是可微的，连续的；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度是改变率或者斜度的另一个称呼。如果你需要回顾这个概念，可以看下可汗学院对这个问题的[讲解](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient)。\n",
    "\n",
    "梯度：https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient\n",
    "\n",
    "避免局部最低点方法:\n",
    "    \n",
    "https://ruder.io/optimizing-gradient-descent/index.html#momentum\n",
    "\n",
    "可汗学院微积分：https://www.khanacademy.org/math/multivariable-calculus\n",
    "\n",
    "向量：https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces/vectors/v/vector-introduction-linear-algebra\n",
    "\n",
    "矩阵：https://www.khanacademy.org/math/precalculus-2018/precalc-matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据清理\n",
    "\n",
    "rank 是类别特征，其中的数字并不表示任何相对的值。排名第 2 并不是排名第 1 的两倍；排名第 3 也不是排名第 2 的 1.5 倍。因此，我们需要用 dummy variables 来对 rank 进行编码。\n",
    "\n",
    "\n",
    "首先你需要初始化权重。我们希望它们比较小，这样输入在 sigmoid 函数那里可以在接近 0 的位置，而不是最高或者最低处。很重要的一点是要随机地初始化它们，这样它们有不同的初始值，是发散且不对称的。所以我们用一个中心为 0 的正态分布来初始化权重，此正态分布的标准差（scale 参数）最好使用 $1/\\sqrt{n}$ ，其中 nn 是输入单元的个数。这样就算是输入单元的数量变多，sigmoid 的输入还能保持比较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 均方差\n",
    "\n",
    "（mean of the square errors，MSE）\n",
    "\n",
    "$$\n",
    "E = \\frac{1}2m\\sum_{\\mu=1}^{m}(y^{\\mu}-\\hat{y}^\\mu)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**梯度下降法**更新权重的算法概述：\n",
    "\n",
    "1.权重步长设定为 0： $\\Delta w_i = 0$\n",
    "\n",
    "2.对训练数据中的每一条记录：\n",
    "\n",
    "a.通过网络做正向传播，计算输出 $\\hat y = f(\\sum_i w_i x_i)$ \n",
    "\n",
    "b.计算输出单元的误差项（error term） $\\delta = (y - \\hat y) * f'(\\sum_i w_i x_i)$\n",
    "\n",
    "c.更新权重步长 $\\Delta w_i = \\Delta w_i + \\delta x_i$\n",
    "\n",
    "d.更新权重 $w_i = w_i + \\eta \\Delta w_i / m$. 其中 $\\eta$ 是学习率， m 是数据点个数。这里我们对权重步长做了平均，为的是降低训练数据中大的变化。\n",
    "\n",
    "3.重复 e 代（epoch）。\n",
    "\n",
    "你也可以对每条记录更新权重，而不是把所有记录都训练过之后再取平均。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们还是使用 sigmoid 作为激活函数\n",
    "\n",
    "$f(h) = 1/(1+e^{-h})$\n",
    "\n",
    "\n",
    "sigmoid 的梯度是： $f'(h) = f(h) (1 - f(h))$\n",
    "\n",
    "\n",
    "其中 h 是输出单元的输入\n",
    "\n",
    "$h = \\sum_i w_i x_i$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用 NumPy 来实现\n",
    "\n",
    "## 初始化\n",
    "首先你需要初始化权重。我们希望它们比较小，这样输入在 sigmoid 函数那里可以在接近 0 的位置，而不是最高或者最低处。很重要的一点是要随机地初始化它们，这样它们有不同的初始值，是发散且不对称的。\n",
    "\n",
    "所以我们用一个中心为 0 的正态分布来初始化权重，此正态分布的标准差（scale 参数）最好使用 $1/\\sqrt{n}$\n",
    "，其中 n 是输入单元的个数。这样就算是输入单元的数量变多，sigmoid 的输入还能保持比较小。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weights = np.random.normal(scale=1/n_features**.5, size=n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "实例，ex5/ex_gd.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对于多层网络**：\n",
    "\n",
    "![](./dl000.png)\n",
    "\n",
    "\n",
    "\n",
    "#Number of records and input units\n",
    "\n",
    "数据点数量以及每个数据点有多少输入节点\n",
    "\n",
    "n_records, n_inputs = features.shape\n",
    "\n",
    "#Number of hidden units\n",
    "\n",
    "#隐藏层节点个数\n",
    "\n",
    "n_hidden = 2\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, n_inputs**-0.5, size=(n_inputs, n_hidden))\n",
    "\n",
    "![](./dl001.png)\n",
    "\n",
    "## 点乘\n",
    "\n",
    "input to the output layer\n",
    "\n",
    "\n",
    "输出层的输入\n",
    "\n",
    "output_in = np.dot(weights, inputs)\n",
    "\n",
    "hidden_inputs = np.dot(inputs, weights_input_to_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output:\n",
      "[0.41492192 0.42604313 0.5002434 ]\n",
      "Output-layer Output:\n",
      "[0.49815196 0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in =  np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in) \n",
    "\n",
    "print('Hidden-layer Output:')\n",
    "print(hidden_layer_out)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out,weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print('Output-layer Output:')\n",
    "print(output_layer_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 反向传播\n",
    "\n",
    "反向传播是训练神经网络的基础原理；\n",
    "\n",
    "反向传播算法梯度下降的一个延伸。以一个两层神经网络为例，可以使用链式法则计算输入层-隐藏层间权重的误差。\n",
    "\n",
    "每层的输出是由两层间的权重决定的，两层之间产生的误差，按权重缩放后在网络中向前传播。既然我们知道输出误差，便可以用权重来反向传播到隐藏层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0===============0===============0===============\n",
      "0.028730669543515018\n",
      "1===============1===============1===============\n",
      "[ 0.1 -0.3]\n",
      "2===============2===============2===============\n",
      "Change in weights for hidden layer to output layer:\n",
      "[0.00804047 0.00555918]\n",
      "Change in weights for input layer to hidden layer:\n",
      "[[ 1.77005547e-04 -5.11178506e-04]\n",
      " [ 3.54011093e-05 -1.02235701e-04]\n",
      " [-7.08022187e-05  2.04471402e-04]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "x = np.array([0.5, 0.1, -0.2])\n",
    "target = 0.6\n",
    "learnrate = 0.5\n",
    "\n",
    "weights_input_hidden = np.array([[0.5, -0.6],\n",
    "                                 [0.1, -0.2],\n",
    "                                 [0.1, 0.7]])\n",
    "\n",
    "weights_hidden_output = np.array([0.1, -0.3])\n",
    "\n",
    "## Forward pass\n",
    "hidden_layer_input = np.dot(x, weights_input_hidden)\n",
    "hidden_layer_output = sigmoid(hidden_layer_input)\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_output, weights_hidden_output)\n",
    "output = sigmoid(output_layer_in)\n",
    "\n",
    "## Backwards pass\n",
    "## TODO: Calculate output error\n",
    "error = target - output\n",
    "\n",
    "# TODO: Calculate error term for output layer\n",
    "output_error_term = error * output * (1 - output)\n",
    "print(\"0===============\"*3)\n",
    "print(output_error_term)\n",
    "print(\"1===============\"*3)\n",
    "print(weights_hidden_output)\n",
    "print(\"2===============\"*3)\n",
    "# TODO: Calculate error term for hidden layer\n",
    "hidden_error_term = np.dot(output_error_term, weights_hidden_output) * \\\n",
    "                    hidden_layer_output * (1 - hidden_layer_output)\n",
    "\n",
    "# TODO: Calculate change in weights for hidden layer to output layer\n",
    "delta_w_h_o = learnrate * output_error_term * hidden_layer_output\n",
    "\n",
    "# TODO: Calculate change in weights for input layer to hidden layer\n",
    "delta_w_i_h = learnrate * hidden_error_term * x[:,None]\n",
    "\n",
    "print('Change in weights for hidden layer to output layer:')\n",
    "print(delta_w_h_o)\n",
    "print('Change in weights for input layer to hidden layer:')\n",
    "print(delta_w_i_h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播（Backpropagation）是深度学习的基础。TensorFlow 或者其它框架会替你把它做好，但是你应该理解它的算法。我们后面还会讲到它，这里有些材料你可以看一下：\n",
    "\n",
    "Andrej Karpathy：是的，你应该了解反向传播:\n",
    "\n",
    "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.vt3ax2kg9\n",
    "\n",
    "同样来自 Andrej Karpathy：斯坦福的 CS231n 课程的一个视频\n",
    "\n",
    "https://www.youtube.com/watch?v=59Hbtz7XgjM\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
